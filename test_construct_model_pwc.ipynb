{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, ZeroPadding2D, Reshape, Concatenate\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from optical_flow_warp_old import transformer_old\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pwc_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, ZeroPadding2D, Reshape, Concatenate\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from optical_flow_warp_old import transformer_old\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pyramid_flow(image):\n",
    "    cnv1 = tf.keras.layers.Conv2D(16, (3, 3), strides=2, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(image)\n",
    "    cnv2 = tf.keras.layers.Conv2D(16, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv1)\n",
    "    cnv3 = tf.keras.layers.Conv2D(32, (3, 3), strides=2, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv2)\n",
    "    cnv4 = tf.keras.layers.Conv2D(32, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv3)\n",
    "    cnv5 = tf.keras.layers.Conv2D(64, (3, 3), strides=2, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv4)\n",
    "    cnv6 = tf.keras.layers.Conv2D(64, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv5)\n",
    "    cnv7 = tf.keras.layers.Conv2D(96, (3, 3), strides=2, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv6)\n",
    "    cnv8 = tf.keras.layers.Conv2D(96, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv7)\n",
    "    cnv9 = tf.keras.layers.Conv2D(128, (3, 3), strides=2, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv8)\n",
    "    cnv10 = tf.keras.layers.Conv2D(128, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv9)\n",
    "    cnv11 = tf.keras.layers.Conv2D(192, (3, 3), strides=2, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv10)\n",
    "    cnv12 = tf.keras.layers.Conv2D(192, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv11)\n",
    "\n",
    "    return cnv2, cnv4, cnv6, cnv8, cnv10, cnv12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_volumn(feature1, feature2, d=4):\n",
    "    batch_size, H, W, feature_num = map(int, feature1.get_shape()[0:4])\n",
    "    feature2 = tf.pad(feature2, [[0, 0], [d, d], [d, d], [0, 0]], \"CONSTANT\")\n",
    "    cv = []\n",
    "    for i in range(2 * d + 1):\n",
    "        for j in range(2 * d + 1):\n",
    "            cv.append(\n",
    "                tf.math.reduce_mean(\n",
    "                    feature1 * feature2[:, i:(i + H), j:(j + W), :],\n",
    "                    axis=3,\n",
    "                    keepdims=True\n",
    "                ))\n",
    "    return tf.concat(cv, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optical_flow_decoder_dc(inputs, level):\n",
    "    cnv1 = tf.keras.layers.Conv2D(128, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(inputs)\n",
    "    cnv2 = tf.keras.layers.Conv2D(128, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(cnv1)\n",
    "    cnv3 = tf.keras.layers.Conv2D(96, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(tf.concat([cnv1, cnv2], axis=3))\n",
    "    cnv4 = tf.keras.layers.Conv2D(64, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(tf.concat([cnv2, cnv3], axis=3))\n",
    "    cnv5 = tf.keras.layers.Conv2D(32, (3, 3), strides=1, padding='same', activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(tf.concat([cnv3, cnv4], axis=3))                                                                                                                                       \n",
    "    flow = tf.keras.layers.Conv2D(2, (3, 3), strides=1, padding='same', activation=None, kernel_regularizer=tf.keras.regularizers.L2(0.0004))(tf.concat([cnv4, cnv5], axis=3))                                                                                                                                       \n",
    "\n",
    "    return flow, cnv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_net(inputs):\n",
    "#     with slim.arg_scope(\n",
    "#         [slim.conv2d, slim.conv2d_transpose],\n",
    "#             weights_regularizer=slim.l2_regularizer(0.0004),\n",
    "#             activation_fn=leaky_relu):\n",
    "#         cnv1 = slim.conv2d(inputs, 128, [3, 3], rate=1, scope=\"cnv1_cn\")\n",
    "        cnv1 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', strides=1, activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004),dilation_rate=(1, 1))(inputs)\n",
    "#         cnv2 = slim.conv2d(cnv1, 128, [3, 3], rate=2, scope=\"cnv2_cn\")\n",
    "        cnv2 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', strides=1, activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004),dilation_rate=(2, 2))(cnv1)\n",
    "#         cnv3 = slim.conv2d(cnv2, 128, [3, 3], rate=4, scope=\"cnv3_cn\")\n",
    "        cnv3 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', strides=1, activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004),dilation_rate=(4, 4))(cnv2)\n",
    "#         cnv4 = slim.conv2d(cnv3, 96, [3, 3], rate=8, scope=\"cnv4_cn\")\n",
    "        cnv4 = tf.keras.layers.Conv2D(96, (3, 3), padding='same', strides=1, activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004),dilation_rate=(8, 8))(cnv3)\n",
    "#         cnv5 = slim.conv2d(cnv4, 64, [3, 3], rate=16, scope=\"cnv5_cn\")\n",
    "        cnv5 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', strides=1, activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004),dilation_rate=(16, 16))(cnv4)\n",
    "#         cnv6 = slim.conv2d(cnv5, 32, [3, 3], rate=1, scope=\"cnv6_cn\")\n",
    "        cnv6 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', strides=1, activation=tf.nn.leaky_relu, kernel_regularizer=tf.keras.regularizers.L2(0.0004),dilation_rate=(1, 1))(cnv5)\n",
    "\n",
    "#         flow = slim.conv2d(cnv6, 2, [3, 3], rate=1, scope=\"cnv7_cn\", activation_fn=None)\n",
    "        flow = tf.keras.layers.Conv2D(2, (3, 3), padding='same', strides=1, activation=None, kernel_regularizer=tf.keras.regularizers.L2(0.0004),dilation_rate=(1, 1))(cnv6)\n",
    "        return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optical_flow_warp_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def transformer_old(U, flo, out_size, name='SpatialTransformer', **kwargs):\n",
    "#     \"\"\"Backward warping layer\n",
    "\n",
    "#     Implements a backward warping layer described in \n",
    "#     \"Unsupervised Deep Learning for Optical Flow Estimation, Zhe Ren et al\"\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     U : float\n",
    "#         The output of a convolutional net should have the\n",
    "#         shape [num_batch, height, width, num_channels].\n",
    "#     flo: float\n",
    "#          The optical flow used to do the backward warping.\n",
    "#          shape is [num_batch, height, width, 2]\n",
    "#     out_size: tuple of two ints\n",
    "#         The size of the output of the network (height, width)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def _repeat(x, n_repeats):\n",
    "#         with tf.compat.v1.variable_scope('_repeat'):\n",
    "#             rep = tf.transpose(\n",
    "#                 tf.expand_dims(\n",
    "#                     tf.ones(shape=tf.stack([n_repeats, ])), 1), [1, 0])\n",
    "#             rep = tf.cast(rep, 'int32')\n",
    "#             x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n",
    "#             return tf.reshape(x, [-1])\n",
    "\n",
    "#     def _interpolate(im, x, y, out_size):\n",
    "#         # constants\n",
    "#         num_batch = tf.shape(im)[0]\n",
    "#         height = tf.shape(im)[1]\n",
    "#         width = tf.shape(im)[2]\n",
    "#         channels = tf.shape(im)[3]\n",
    "\n",
    "#         x = tf.cast(x, 'float32')\n",
    "#         y = tf.cast(y, 'float32')\n",
    "#         height_f = tf.cast(height, 'float32')\n",
    "#         width_f = tf.cast(width, 'float32')\n",
    "#         out_height = out_size[0]\n",
    "#         out_width = out_size[1]\n",
    "#         zero = tf.zeros([], dtype='int32')\n",
    "#         max_y = tf.cast(tf.shape(im)[1] - 1, 'int32')\n",
    "#         max_x = tf.cast(tf.shape(im)[2] - 1, 'int32')\n",
    "\n",
    "#         # scale indices from [-1, 1] to [0, width/height]\n",
    "#         x = (x + 1.0) * (width_f - 1) / 2.0\n",
    "#         y = (y + 1.0) * (height_f - 1) / 2.0\n",
    "\n",
    "#         # do sampling\n",
    "#         x0 = tf.cast(tf.floor(x), 'int32')\n",
    "#         x1 = x0 + 1\n",
    "#         y0 = tf.cast(tf.floor(y), 'int32')\n",
    "#         y1 = y0 + 1\n",
    "\n",
    "#         x0_c = tf.clip_by_value(x0, zero, max_x)\n",
    "#         x1_c = tf.clip_by_value(x1, zero, max_x)\n",
    "#         y0_c = tf.clip_by_value(y0, zero, max_y)\n",
    "#         y1_c = tf.clip_by_value(y1, zero, max_y)\n",
    "\n",
    "#         dim2 = width\n",
    "#         dim1 = width * height\n",
    "#         base = _repeat(tf.range(num_batch) * dim1, out_height * out_width)\n",
    "\n",
    "#         base_y0 = base + y0_c * dim2\n",
    "#         base_y1 = base + y1_c * dim2\n",
    "#         idx_a = base_y0 + x0_c\n",
    "#         idx_b = base_y1 + x0_c\n",
    "#         idx_c = base_y0 + x1_c\n",
    "#         idx_d = base_y1 + x1_c\n",
    "\n",
    "#         # use indices to lookup pixels in the flat image and restore\n",
    "#         # channels dim\n",
    "#         im_flat = tf.reshape(im, tf.stack([-1, channels]))\n",
    "#         im_flat = tf.cast(im_flat, 'float32')\n",
    "#         Ia = tf.gather(im_flat, idx_a)\n",
    "#         Ib = tf.gather(im_flat, idx_b)\n",
    "#         Ic = tf.gather(im_flat, idx_c)\n",
    "#         Id = tf.gather(im_flat, idx_d)\n",
    "\n",
    "#         # and finally calculate interpolated values\n",
    "#         x0_f = tf.cast(x0, 'float32')\n",
    "#         x1_f = tf.cast(x1, 'float32')\n",
    "#         y0_f = tf.cast(y0, 'float32')\n",
    "#         y1_f = tf.cast(y1, 'float32')\n",
    "#         wa = tf.expand_dims(((x1_f - x) * (y1_f - y)), 1)\n",
    "#         wb = tf.expand_dims(((x1_f - x) * (y - y0_f)), 1)\n",
    "#         wc = tf.expand_dims(((x - x0_f) * (y1_f - y)), 1)\n",
    "#         wd = tf.expand_dims(((x - x0_f) * (y - y0_f)), 1)\n",
    "#         output = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])\n",
    "#         return output\n",
    "\n",
    "#     def _meshgrid(height, width):\n",
    "#         # This should be equivalent to:\n",
    "#         #  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),\n",
    "#         #                         np.linspace(-1, 1, height))\n",
    "#         #  ones = np.ones(np.prod(x_t.shape))\n",
    "#         #  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])\n",
    "#         x_t = tf.matmul(\n",
    "#             tf.ones(shape=tf.stack([int(height), 1])),\n",
    "#             tf.transpose(\n",
    "#                 tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), [1, 0]))\n",
    "#         y_t = tf.matmul(\n",
    "#             tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1),\n",
    "#             tf.ones(shape=tf.stack([1, width])))\n",
    "\n",
    "#         return x_t, y_t\n",
    "\n",
    "#     def _transform(flo, input_dim, out_size):\n",
    "#         with tf.compat.v1.variable_scope('_transform'):\n",
    "#             num_batch = tf.shape(input_dim)[0]\n",
    "#             height = tf.shape(input_dim)[1]\n",
    "#             width = tf.shape(input_dim)[2]\n",
    "#             num_channels = tf.shape(input_dim)[3]\n",
    "\n",
    "#             # grid of (x_t, y_t, 1), eq (1) in ref [1]\n",
    "#             height_f = tf.cast(height, 'float32')\n",
    "#             width_f = tf.cast(width, 'float32')\n",
    "#             out_height = out_size[0]\n",
    "#             out_width = out_size[1]\n",
    "#             x_t, y_t = _meshgrid(out_height, out_width)\n",
    "#             x_t = tf.expand_dims(x_t, 0)\n",
    "#             x_t = tf.tile(x_t, [num_batch, 1, 1])\n",
    "\n",
    "#             y_t = tf.expand_dims(y_t, 0)\n",
    "#             y_t = tf.tile(y_t, [num_batch, 1, 1])\n",
    "\n",
    "#             x_s = x_t + flo[:, :, :, 0] / (\n",
    "#                 (tf.cast(out_width, tf.float32) - 1.0) / 2.0)\n",
    "#             y_s = y_t + flo[:, :, :, 1] / (\n",
    "#                 (tf.cast(out_height, tf.float32) - 1.0) / 2.0)\n",
    "\n",
    "#             x_s_flat = tf.reshape(x_s, [-1])\n",
    "#             y_s_flat = tf.reshape(y_s, [-1])\n",
    "\n",
    "#             input_transformed = _interpolate(input_dim, x_s_flat, y_s_flat,\n",
    "#                                              out_size)\n",
    "\n",
    "#             output = tf.reshape(\n",
    "#                 input_transformed,\n",
    "#                 tf.stack([num_batch, out_height, out_width, num_channels]))\n",
    "#             return output\n",
    "\n",
    "#     with tf.compat.v1.variable_scope(name):\n",
    "#         output = _transform(flo, U, out_size)\n",
    "#         return output\n",
    "\n",
    "\n",
    "# # def main(unused_argv):\n",
    "# #     sess = tf.Session(config=tf.ConfigProto(\n",
    "# #         allow_soft_placement=True, log_device_placement=False))\n",
    "\n",
    "# #     image = tf.constant(\n",
    "# #         [1, 2, 3, 4, 5, 6, 7, 8, 9], shape=[1, 3, 3, 1], dtype=\"float32\")\n",
    "\n",
    "# #     flo = np.zeros((1, 3, 3, 2))\n",
    "# #     flo[0, 1, 1, 0] = 1.0\n",
    "# #     #flo[0, 1, 1, 1] = 1.0\n",
    "# #     flo = tf.constant(flo, dtype=\"float32\")\n",
    "\n",
    "# #     image2 = transformer_old(image, flo, [3, 3])\n",
    "\n",
    "# #     print(image2.eval(session=sess))\n",
    "\n",
    "\n",
    "# # if __name__ == '__main__':\n",
    "# #     app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pwc_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model_pwc_full(image1, image2, feature1, feature2):\n",
    "# with tf.variable_scope('flow_net'):\n",
    "    batch_size, H, W, color_channels = map(int, image1.get_shape()[0:4])\n",
    "\n",
    "    #############################\n",
    "    feature1_1, feature1_2, feature1_3, feature1_4, feature1_5, feature1_6 = feature1\n",
    "    feature2_1, feature2_2, feature2_3, feature2_4, feature2_5, feature2_6 = feature2\n",
    "\n",
    "    cv6 = cost_volumn(feature1_6, feature2_6, d=4)\n",
    "    flow6, _ = optical_flow_decoder_dc(cv6, level=6)\n",
    "\n",
    "    flow6to5 = tf.compat.v1.image.resize_bilinear(flow6,\n",
    "                                        [H // (2**5), (W // (2**5))]) * 2.0\n",
    "    \n",
    "    feature2_5w = transformer_old(feature2_5, flow6to5, [H // 32, W // 32])\n",
    "    cv5 = cost_volumn(feature1_5, feature2_5w, d=4)\n",
    "    flow5, _ = optical_flow_decoder_dc(\n",
    "        tf.concat(\n",
    "            [cv5, feature1_5, flow6to5], axis=3), level=5)\n",
    "    flow5 = flow5 + flow6to5\n",
    "\n",
    "    flow5to4 = tf.compat.v1.image.resize_bilinear(flow5,\n",
    "                                        [H // (2**4), (W // (2**4))]) * 2.0\n",
    "    feature2_4w = transformer_old(feature2_4, flow5to4, [H // 16, W // 16])\n",
    "    cv4 = cost_volumn(feature1_4, feature2_4w, d=4)\n",
    "    flow4, _ = optical_flow_decoder_dc(\n",
    "        tf.concat(\n",
    "            [cv4, feature1_4, flow5to4], axis=3), level=4)\n",
    "    flow4 = flow4 + flow5to4\n",
    "\n",
    "    flow4to3 = tf.compat.v1.image.resize_bilinear(flow4,\n",
    "                                        [H // (2**3), (W // (2**3))]) * 2.0\n",
    "    feature2_3w = transformer_old(feature2_3, flow4to3, [H // 8, W // 8])\n",
    "    cv3 = cost_volumn(feature1_3, feature2_3w, d=4)\n",
    "    flow3, _ = optical_flow_decoder_dc(\n",
    "        tf.concat(\n",
    "            [cv3, feature1_3, flow4to3], axis=3), level=3)\n",
    "    flow3 = flow3 + flow4to3\n",
    "\n",
    "    flow3to2 = tf.compat.v1.image.resize_bilinear(flow3,\n",
    "                                        [H // (2**2), (W // (2**2))]) * 2.0\n",
    "    feature2_2w = transformer_old(feature2_2, flow3to2, [H // 4, W // 4])\n",
    "    cv2 = cost_volumn(feature1_2, feature2_2w, d=4)\n",
    "    flow2_raw, f2 = optical_flow_decoder_dc(\n",
    "        tf.concat(\n",
    "            [cv2, feature1_2, flow3to2], axis=3), level=2)\n",
    "    flow2_raw = flow2_raw + flow3to2\n",
    "\n",
    "    flow2 = context_net(tf.concat([flow2_raw, f2], axis=3)) + flow2_raw\n",
    "\n",
    "    flow0_enlarge = tf.compat.v1.image.resize_bilinear(flow2 * 4.0, [H, W])\n",
    "    flow1_enlarge = tf.compat.v1.image.resize_bilinear(flow3 * 4.0, [H // 2, W // 2])\n",
    "    flow2_enlarge = tf.compat.v1.image.resize_bilinear(flow4 * 4.0, [H // 4, W // 4])\n",
    "    flow3_enlarge = tf.compat.v1.image.resize_bilinear(flow5 * 4.0, [H // 8, W // 8])\n",
    "\n",
    "    return flow0_enlarge, flow1_enlarge, flow2_enlarge, flow3_enlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test construct_model_pwc_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_parse import fake_parse\n",
    "from dataloader_reimplement import generator_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = fake_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generator_train(opt),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None,None,3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,None,3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,None,3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,None,3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,None,3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,None,3), dtype=tf.float32),\n",
    "    ))\n",
    "my_dataset = my_dataset.batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in my_dataset:\n",
    "    left_image_o, right_image_o, next_left_image_o, next_right_image_o, proj_cam2pix, proj_pix2cam = element\n",
    "    feature1 = feature_pyramid_flow(left_image_o)\n",
    "#     print(feature1.get_shape())\n",
    "    feature2 = feature_pyramid_flow(right_image_o)\n",
    "#     print(feature2.get_shape())\n",
    "    flow0_enlarge, flow1_enlarge, flow2_enlarge, flow3_enlarge = construct_model_pwc_full(left_image_o, right_image_o, feature1, feature2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(cnv2.shape)\n",
    "#     print(cnv4.shape)\n",
    "#     print(cnv6.shape)\n",
    "#     print(cnv8.shape)\n",
    "#     print(cnv10.shape)\n",
    "#     print(cnv12.shape)\n",
    "#     print(\"_________\")\n",
    "\n",
    "#     def train(model, dataset, optimizer):\n",
    "#     for step, (x1, x2, y) in enumerate(dataset):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             left, right = model([x1, x2])\n",
    "#             loss = contrastive_loss(left, right, tf.cast(y, tf.float32))\n",
    "#         gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
