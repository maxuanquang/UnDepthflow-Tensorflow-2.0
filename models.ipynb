{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\quangmx\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "# from nets.pose_net import pose_exp_net\n",
    "# from monodepth_model import disp_godard\n",
    "from nets.pwc_flow import construct_model_pwc_full, feature_pyramid_flow\n",
    "# from nets.pwc_disp import feature_pyramid_disp\n",
    "# from optical_flow_warp_fwd import transformerFwd\n",
    "# from optical_flow_warp_old import transformer_old\n",
    "from monodepth_dataloader import get_multi_scale_intrinsics\n",
    "# from utils import inverse_warp, inverse_warp_new\n",
    "# from loss_utils import SSIM, deprocess_image, preprocess_image,\\\n",
    "#   cal_grad2_error_mask, charbonnier_loss, cal_grad2_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MonodepthDataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-651994374b55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMonodepthDataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'MonodepthDataloader' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader = MonodepthDataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_flow(object):\n",
    "    def __init__(self,\n",
    "                 image1=None,\n",
    "                 image2=None,\n",
    "                 image1r=None,\n",
    "                 image2r=None,\n",
    "                 cam2pix=None,\n",
    "                 pix2cam=None,\n",
    "                 reuse_scope=False,\n",
    "                 scope=None):\n",
    "        summaries = []\n",
    "\n",
    "        batch_size, H, W, color_channels = map(int, image1.get_shape()[0:4])\n",
    "\n",
    "        with tf.compat.v1.variable_scope(scope, reuse=reuse_scope):\n",
    "            feature1 = feature_pyramid_flow(image1, reuse=False)\n",
    "            feature2 = feature_pyramid_flow(image2, reuse=True)\n",
    "\n",
    "            optical_flows = construct_model_pwc_full(image1, image2, feature1,\n",
    "                                                     feature2)\n",
    "\n",
    "        with tf.compat.v1.variable_scope(scope, reuse=True):\n",
    "            optical_flows_rev = construct_model_pwc_full(image2, image1,\n",
    "                                                         feature2, feature1)\n",
    "\n",
    "        occu_masks = [\n",
    "            tf.clip_by_value(\n",
    "                transformerFwd(\n",
    "                    tf.ones(\n",
    "                        shape=[batch_size, H / (2**s), W / (2**s), 1],\n",
    "                        dtype='float32'),\n",
    "                    flowr, [H / (2**s), W / (2**s)]),\n",
    "                clip_value_min=0.0,\n",
    "                clip_value_max=1.0)\n",
    "            for s, flowr in enumerate(optical_flows_rev)\n",
    "        ]\n",
    "\n",
    "        pixel_loss_depth = 0\n",
    "        pixel_loss_optical = 0\n",
    "        exp_loss = 0\n",
    "        flow_smooth_loss = 0\n",
    "        tgt_image_all = []\n",
    "        src_image_all = []\n",
    "        proj_image_depth_all = []\n",
    "        proj_error_depth_all = []\n",
    "        exp_mask_stack_all = []\n",
    "        flyout_map_all = []\n",
    "\n",
    "        for s in range(opt.num_scales):\n",
    "            # Scale the source and target images for computing loss at the \n",
    "            # according scale.\n",
    "            curr_tgt_image = tf.image.resize(\n",
    "                image1,\n",
    "                [int(opt.img_height / (2**s)), int(opt.img_width / (2**s))], method=tf.image.ResizeMethod.AREA)\n",
    "            curr_src_image = tf.image.resize(\n",
    "                image2,\n",
    "                [int(opt.img_height / (2**s)), int(opt.img_width / (2**s))], method=tf.image.ResizeMethod.AREA)\n",
    "\n",
    "            occu_mask = occu_masks[s]\n",
    "            occu_mask_avg = tf.reduce_mean(input_tensor=occu_mask)\n",
    "\n",
    "            curr_proj_image_optical = transformer_old(\n",
    "                curr_src_image, optical_flows[s], [H / (2**s), W / (2**s)])\n",
    "            curr_proj_error_optical = tf.abs(curr_proj_image_optical -\n",
    "                                             curr_tgt_image)\n",
    "            pixel_loss_optical += (1.0 - opt.ssim_weight) * tf.reduce_mean(\n",
    "                input_tensor=curr_proj_error_optical * occu_mask) / occu_mask_avg\n",
    "\n",
    "            curr_flyout_map = occu_mask\n",
    "\n",
    "            if opt.ssim_weight > 0:\n",
    "                pixel_loss_optical += opt.ssim_weight * tf.reduce_mean(\n",
    "                    input_tensor=SSIM(curr_proj_image_optical * occu_mask, curr_tgt_image *\n",
    "                         occu_mask)) / occu_mask_avg\n",
    "\n",
    "            flow_smooth_loss += opt.flow_smooth_weight * cal_grad2_error(\n",
    "                optical_flows[s] / 20.0, curr_tgt_image, 1.0)\n",
    "\n",
    "            tgt_image_all.append(curr_tgt_image)\n",
    "            src_image_all.append(curr_src_image)\n",
    "            proj_image_depth_all.append(curr_proj_image_optical)\n",
    "            proj_error_depth_all.append(curr_proj_error_optical)\n",
    "\n",
    "            flyout_map_all.append(curr_flyout_map)\n",
    "\n",
    "        self.loss = (pixel_loss_optical + flow_smooth_loss)\n",
    "\n",
    "        summaries.append(tf.compat.v1.summary.scalar(\"total_loss\", self.loss))\n",
    "        summaries.append(\n",
    "            tf.compat.v1.summary.scalar(\"pixel_loss_depth\", pixel_loss_depth))\n",
    "        summaries.append(\n",
    "            tf.compat.v1.summary.scalar(\"pixel_loss_optical\", pixel_loss_optical))\n",
    "        summaries.append(tf.compat.v1.summary.scalar(\"exp_loss\", exp_loss))\n",
    "        tf.compat.v1.summary.image('scale%d_target_image' % s, \\\n",
    "                         deprocess_image(tgt_image_all[s]))\n",
    "        tf.compat.v1.summary.image('scale%d_src_image' % s, \\\n",
    "                         deprocess_image(src_image_all[s]))\n",
    "\n",
    "        tf.compat.v1.summary.image('scale_projected_image',\n",
    "                         deprocess_image(proj_image_depth_all[s]))\n",
    "        tf.compat.v1.summary.image('scale_proj_error_error', proj_error_depth_all[s])\n",
    "        tf.compat.v1.summary.image('scale_flyout_mask', flyout_map_all[s])\n",
    "\n",
    "        self.summ_op = tf.compat.v1.summary.merge(summaries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_eval_flow(object):\n",
    "    def __init__(self, scope=None):\n",
    "        with tf.compat.v1.variable_scope(scope, reuse=True):\n",
    "            input_uint8_1 = tf.compat.v1.placeholder(\n",
    "                tf.uint8, [1, opt.img_height, opt.img_width, 3],\n",
    "                name='raw_input_1')\n",
    "            input_uint8_1r = tf.compat.v1.placeholder(\n",
    "                tf.uint8, [1, opt.img_height, opt.img_width, 3],\n",
    "                name='raw_input_1r')\n",
    "            input_uint8_2 = tf.compat.v1.placeholder(\n",
    "                tf.uint8, [1, opt.img_height, opt.img_width, 3],\n",
    "                name='raw_input_2')\n",
    "            input_uint8_2r = tf.compat.v1.placeholder(\n",
    "                tf.uint8, [1, opt.img_height, opt.img_width, 3],\n",
    "                name='raw_input_2r')\n",
    "            input_intrinsic = tf.compat.v1.placeholder(tf.float32, [3, 3])\n",
    "\n",
    "            cam2pix, pix2cam = get_multi_scale_intrinsics(input_intrinsic,\n",
    "                                                          opt.num_scales)\n",
    "            cam2pix = tf.expand_dims(cam2pix, axis=0)\n",
    "            pix2cam = tf.expand_dims(pix2cam, axis=0)\n",
    "\n",
    "            input_1 = preprocess_image(input_uint8_1)\n",
    "            input_2 = preprocess_image(input_uint8_2)\n",
    "            input_1r = preprocess_image(input_uint8_1r)\n",
    "            input_2r = preprocess_image(input_uint8_2r)\n",
    "\n",
    "            feature1 = feature_pyramid_flow(input_1, reuse=True)\n",
    "            feature2 = feature_pyramid_flow(input_2, reuse=True)\n",
    "\n",
    "            optical_flows = construct_model_pwc_full(input_1, input_2,\n",
    "                                                     feature1, feature2)\n",
    "\n",
    "        self.input_1 = input_uint8_1\n",
    "        self.input_2 = input_uint8_2\n",
    "        self.input_r = input_uint8_1r\n",
    "        self.input_2r = input_uint8_2r\n",
    "        self.input_intrinsic = input_intrinsic\n",
    "        self.pred_flow_optical = optical_flows[0]\n",
    "\n",
    "        # Placeholder created for interface consistency\n",
    "        self.pred_flow_rigid = tf.constant(0.0)\n",
    "        self.pred_disp = tf.constant(0.0)\n",
    "        self.pred_disp2 = tf.constant(0.0)\n",
    "        self.pred_mask = tf.constant(0.0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
