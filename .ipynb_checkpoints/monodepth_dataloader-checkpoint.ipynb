{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_length_tf(t):\n",
    "    return tf.compat.v1.py_func(len, [t], [tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_intrinsics(raw_cam_mat, opt, orig_height, orig_width):\n",
    "    fx = raw_cam_mat[0, 0]\n",
    "    fy = raw_cam_mat[1, 1]\n",
    "    cx = raw_cam_mat[0, 2]\n",
    "    cy = raw_cam_mat[1, 2]\n",
    "    r1 = tf.stack(\n",
    "        [fx * opt.img_width / orig_width, 0, cx * opt.img_width / orig_width])\n",
    "    r2 = tf.stack([\n",
    "        0, fy * opt.img_height / orig_height, cy * opt.img_height / orig_height\n",
    "    ])\n",
    "    r3 = tf.constant([0., 0., 1.])\n",
    "    return tf.stack([r1, r2, r3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_scale_intrinsics(raw_cam_mat, num_scales):\n",
    "    proj_cam2pix = []\n",
    "    # Scale the intrinsics accordingly for each scale\n",
    "    for s in range(num_scales):\n",
    "        fx = raw_cam_mat[0, 0] / (2**s)\n",
    "        fy = raw_cam_mat[1, 1] / (2**s)\n",
    "        cx = raw_cam_mat[0, 2] / (2**s)\n",
    "        cy = raw_cam_mat[1, 2] / (2**s)\n",
    "        r1 = tf.stack([fx, 0, cx])\n",
    "        r2 = tf.stack([0, fy, cy])\n",
    "        r3 = tf.constant([0., 0., 1.])\n",
    "        proj_cam2pix.append(tf.stack([r1, r2, r3]))\n",
    "    proj_cam2pix = tf.stack(proj_cam2pix)\n",
    "    proj_pix2cam = tf.linalg.inv(proj_cam2pix)\n",
    "    proj_cam2pix.set_shape([num_scales, 3, 3])\n",
    "    proj_pix2cam.set_shape([num_scales, 3, 3])\n",
    "    return proj_cam2pix, proj_pix2cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_intrinsics_matrix(fx, fy, cx, cy):\n",
    "    # Assumes batch input\n",
    "    batch_size = fx.get_shape().as_list()[0]\n",
    "    zeros = tf.zeros_like(fx)\n",
    "    r1 = tf.stack([fx, zeros, cx], axis=1)\n",
    "    r2 = tf.stack([zeros, fy, cy], axis=1)\n",
    "    r3 = tf.constant([0., 0., 1.], shape=[1, 3])\n",
    "    r3 = tf.tile(r3, [batch_size, 1])\n",
    "    intrinsics = tf.stack([r1, r2, r3], axis=1)\n",
    "    return intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(im, intrinsics, out_h, out_w):\n",
    "    # Random scaling\n",
    "    def random_scaling(im, intrinsics):\n",
    "        batch_size, in_h, in_w, _ = im.get_shape().as_list()\n",
    "        scaling = tf.random.uniform([2], 1, 1.15)\n",
    "        x_scaling = scaling[0]\n",
    "        y_scaling = scaling[1]\n",
    "        out_h = tf.cast(in_h * y_scaling, dtype=tf.int32)\n",
    "        out_w = tf.cast(in_w * x_scaling, dtype=tf.int32)\n",
    "        im = tf.compat.v1.image.resize(im, [out_h, out_w], method=tf.compat.v1.image.ResizeMethod.AREA)\n",
    "        fx = intrinsics[:, 0, 0] * x_scaling\n",
    "        fy = intrinsics[:, 1, 1] * y_scaling\n",
    "        cx = intrinsics[:, 0, 2] * x_scaling\n",
    "        cy = intrinsics[:, 1, 2] * y_scaling\n",
    "        intrinsics = make_intrinsics_matrix(fx, fy, cx, cy)\n",
    "        return im, intrinsics\n",
    "\n",
    "    # Random cropping\n",
    "    def random_cropping(im, intrinsics, out_h, out_w):\n",
    "        # batch_size, in_h, in_w, _ = im.get_shape().as_list()\n",
    "        batch_size, in_h, in_w, _ = tf.unstack(tf.shape(input=im))\n",
    "        offset_y = tf.random.uniform(\n",
    "            [1], 0, in_h - out_h + 1, dtype=tf.int32)[0]\n",
    "        offset_x = tf.random.uniform(\n",
    "            [1], 0, in_w - out_w + 1, dtype=tf.int32)[0]\n",
    "        im = tf.image.crop_to_bounding_box(im, offset_y, offset_x, out_h,\n",
    "                                           out_w)\n",
    "        fx = intrinsics[:, 0, 0]\n",
    "        fy = intrinsics[:, 1, 1]\n",
    "        cx = intrinsics[:, 0, 2] - tf.cast(offset_x, dtype=tf.float32)\n",
    "        cy = intrinsics[:, 1, 2] - tf.cast(offset_y, dtype=tf.float32)\n",
    "        intrinsics = make_intrinsics_matrix(fx, fy, cx, cy)\n",
    "        return im, intrinsics\n",
    "\n",
    "    im, intrinsics = random_scaling(im, intrinsics)\n",
    "    im, intrinsics = random_cropping(im, intrinsics, out_h, out_w)\n",
    "    return im, intrinsics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_file = opt.train_file\n",
    "input_queue = tf.data.Dataset.from_tensor_slices([filenames_file])\n",
    "line_reader = tf.data.TextLineDataset(input_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28968"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(line_reader.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'./filenames/kitti_train_files_png_4frames.txt', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for name in input_queue:\n",
    "    print(name)\n",
    "    i+=1\n",
    "    if i==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = opt.data_dir\n",
    "filenames_file = opt.train_file\n",
    "\n",
    "with open(filenames_file, 'r') as f:\n",
    "    input_queue = f.readlines()\n",
    "\n",
    "input_queue\n",
    "\n",
    "split_line = [line.split() for line in input_queue[:-1]]\n",
    "\n",
    "split_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2011_09_26/2011_09_26_drive_0020_sync/image_02/data/0000000043.png',\n",
       "  '2011_09_26/2011_09_26_drive_0020_sync/image_03/data/0000000043.png',\n",
       "  '2011_09_26/2011_09_26_drive_0020_sync/image_02/data/0000000044.png',\n",
       "  '2011_09_26/2011_09_26_drive_0020_sync/image_03/data/0000000044.png',\n",
       "  '2011_09_26/calib_cam_to_cam.txt'],\n",
       " ['2011_09_26/2011_09_26_drive_0060_sync/image_02/data/0000000065.png',\n",
       "  '2011_09_26/2011_09_26_drive_0060_sync/image_03/data/0000000065.png',\n",
       "  '2011_09_26/2011_09_26_drive_0060_sync/image_02/data/0000000066.png',\n",
       "  '2011_09_26/2011_09_26_drive_0060_sync/image_03/data/0000000066.png',\n",
       "  '2011_09_26/calib_cam_to_cam.txt'],\n",
       " ['2011_09_26/2011_09_26_drive_0001_sync/image_02/data/0000000085.png',\n",
       "  '2011_09_26/2011_09_26_drive_0001_sync/image_03/data/0000000085.png',\n",
       "  '2011_09_26/2011_09_26_drive_0001_sync/image_02/data/0000000086.png',\n",
       "  '2011_09_26/2011_09_26_drive_0001_sync/image_03/data/0000000086.png',\n",
       "  '2011_09_26/calib_cam_to_cam.txt']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwuang//2011_09_26/2011_09_26_drive_0020_sync/image_02/data/0000000043.png'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"//\".join(['qwuang',split_line[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image_path = [r\"/\".join([data_path, split_line[i][0]]) for i in range(len(split_line))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kitti_data/2011_09_26/2011_09_26_drive_0020_sync/image_02/data/0000000043.png',\n",
       " 'kitti_data/2011_09_26/2011_09_26_drive_0060_sync/image_02/data/0000000065.png',\n",
       " 'kitti_data/2011_09_26/2011_09_26_drive_0001_sync/image_02/data/0000000085.png']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonodepthDataloader(object):\n",
    "    \"\"\"monodepth dataloader\"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self.data_path = opt.data_dir\n",
    "        self.opt = opt\n",
    "        filenames_file = opt.train_file\n",
    "        \n",
    "        with open(filenames_file, 'r') as f:\n",
    "            input_queue = f.readlines()\n",
    "        split_line = [line.split() for line in input_queue[:-1]]\n",
    "\n",
    "        # we load only one image for test, except if we trained a stereo model   \n",
    "        left_image_path = [r\"/\".join([data_path, split_line[i][0]]) for i in range(len(split_line))]\n",
    "        right_image_path = [r\"/\".join([data_path, split_line[i][1]]) for i in range(len(split_line))] \n",
    "        next_left_image_path = [r\"/\".join([data_path, split_line[i][2]]) for i in range(len(split_line))]\n",
    "        next_right_image_path = [r\"/\".join([data_path, split_line[i][3]]) for i in range(len(split_line))]\n",
    "        cam_intrinsic_path = [r\"/\".join([data_path, split_line[i][4]]) for i in range(len(split_line))]\n",
    "        \n",
    "        left_image_o, orig_height, orig_width = self.read_image(\n",
    "            left_image_path, get_shape=True)\n",
    "        right_image_o = self.read_image(right_image_path)\n",
    "        next_left_image_o = self.read_image(next_left_image_path)\n",
    "        next_right_image_o = self.read_image(next_right_image_path)\n",
    "\n",
    "        # randomly flip images\n",
    "        do_flip = tf.random.uniform([], 0, 1)\n",
    "        left_image = tf.cond(pred=do_flip > 0.5,\n",
    "                             true_fn=lambda: tf.image.flip_left_right(right_image_o),\n",
    "                             false_fn=lambda: left_image_o)\n",
    "        right_image = tf.cond(pred=do_flip > 0.5,\n",
    "                              true_fn=lambda: tf.image.flip_left_right(left_image_o),\n",
    "                              false_fn=lambda: right_image_o)\n",
    "\n",
    "        next_left_image = tf.cond(\n",
    "            pred=do_flip > 0.5,\n",
    "            true_fn=lambda: tf.image.flip_left_right(next_right_image_o),\n",
    "            false_fn=lambda: next_left_image_o)\n",
    "        next_right_image = tf.cond(\n",
    "            pred=do_flip > 0.5, true_fn=lambda: tf.image.flip_left_right(next_left_image_o),\n",
    "            false_fn=lambda: next_right_image_o)\n",
    "\n",
    "        do_flip_fb = tf.random.uniform([], 0, 1)\n",
    "        left_image, right_image, next_left_image, next_right_image = tf.cond(\n",
    "            pred=do_flip_fb > 0.5,\n",
    "            true_fn=lambda: (next_left_image, next_right_image, left_image, right_image),\n",
    "            false_fn=lambda: (left_image, right_image, next_left_image, next_right_image)\n",
    "        )\n",
    "\n",
    "        # randomly augment images\n",
    "        #         do_augment  = tf.random_uniform([], 0, 0)\n",
    "        #         image_list = [left_image, right_image, next_left_image, next_right_image]\n",
    "        #         left_image, right_image, next_left_image, next_right_image = tf.cond(do_augment > 0.5, \n",
    "        #                                                                              lambda: self.augment_image_list(image_list), \n",
    "        #                                                                              lambda: image_list)\n",
    "\n",
    "        left_image.set_shape([None, None, 3])\n",
    "        right_image.set_shape([None, None, 3])\n",
    "        next_left_image.set_shape([None, None, 3])\n",
    "        next_right_image.set_shape([None, None, 3])\n",
    "\n",
    "        raw_cam_contents = tf.io.read_file(cam_intrinsic_path)\n",
    "        last_line = tf.compat.v1.string_split(\n",
    "            [raw_cam_contents], delimiter=\"\\n\").values[-1]\n",
    "        raw_cam_vec = tf.strings.to_number(\n",
    "            tf.compat.v1.string_split([last_line]).values[1:])\n",
    "        raw_cam_mat = tf.reshape(raw_cam_vec, [3, 4])\n",
    "        raw_cam_mat = raw_cam_mat[0:3, 0:3]\n",
    "        raw_cam_mat = rescale_intrinsics(raw_cam_mat, opt, orig_height,\n",
    "                                         orig_width)\n",
    "\n",
    "        # Scale and crop augmentation\n",
    "        #         im_batch = tf.concat([tf.expand_dims(left_image, 0), \n",
    "        #                          tf.expand_dims(right_image, 0),\n",
    "        #                          tf.expand_dims(next_left_image, 0),\n",
    "        #                          tf.expand_dims(next_right_image, 0)], axis=3)\n",
    "        #         raw_cam_mat_batch = tf.expand_dims(raw_cam_mat, axis=0)\n",
    "        #         im_batch, raw_cam_mat_batch = data_augmentation(im_batch, raw_cam_mat_batch, self.opt.img_height, self.opt.img_width)\n",
    "        #         left_image, right_image, next_left_image, next_right_image = tf.split(im_batch[0,:,:,:], num_or_size_splits=4, axis=2)\n",
    "        #         raw_cam_mat = raw_cam_mat_batch[0,:,:]\n",
    "\n",
    "        proj_cam2pix, proj_pix2cam = get_multi_scale_intrinsics(raw_cam_mat,\n",
    "                                                                opt.num_scales)\n",
    "\n",
    "        # capacity = min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "        min_after_dequeue = 2048\n",
    "        capacity = min_after_dequeue + 4 * opt.batch_size\n",
    "        self.data_batch = tf.compat.v1.train.shuffle_batch([\n",
    "            left_image, right_image, next_left_image, next_right_image,\n",
    "            proj_cam2pix, proj_pix2cam\n",
    "        ], opt.batch_size, capacity, min_after_dequeue, 10)\n",
    "\n",
    "    def augment_image_pair(self, left_image, right_image):\n",
    "        # randomly shift gamma\n",
    "        random_gamma = tf.random.uniform([], 0.8, 1.2)\n",
    "        left_image_aug = left_image**random_gamma\n",
    "        right_image_aug = right_image**random_gamma\n",
    "\n",
    "        # randomly shift brightness\n",
    "        random_brightness = tf.random.uniform([], 0.5, 2.0)\n",
    "        left_image_aug = left_image_aug * random_brightness\n",
    "        right_image_aug = right_image_aug * random_brightness\n",
    "\n",
    "        # randomly shift color\n",
    "        random_colors = tf.random.uniform([3], 0.8, 1.2)\n",
    "        white = tf.ones([tf.shape(input=left_image)[0], tf.shape(input=left_image)[1]])\n",
    "        color_image = tf.stack(\n",
    "            [white * random_colors[i] for i in range(3)], axis=2)\n",
    "        left_image_aug *= color_image\n",
    "        right_image_aug *= color_image\n",
    "\n",
    "        # saturate\n",
    "        left_image_aug = tf.clip_by_value(left_image_aug, 0, 1)\n",
    "        right_image_aug = tf.clip_by_value(right_image_aug, 0, 1)\n",
    "\n",
    "        return left_image_aug, right_image_aug\n",
    "\n",
    "    def augment_image_list(self, image_list):\n",
    "        # randomly shift gamma\n",
    "        random_gamma = tf.random.uniform([], 0.8, 1.2)\n",
    "        image_list = [img**random_gamma for img in image_list]\n",
    "\n",
    "        # randomly shift brightness\n",
    "        random_brightness = tf.random.uniform([], 0.5, 2.0)\n",
    "        image_list = [img * random_brightness for img in image_list]\n",
    "\n",
    "        # randomly shift color\n",
    "        random_colors = tf.random.uniform([3], 0.8, 1.2)\n",
    "        white = tf.ones(\n",
    "            [tf.shape(input=image_list[0])[0], tf.shape(input=image_list[0])[1]])\n",
    "        color_image = tf.stack(\n",
    "            [white * random_colors[i] for i in range(3)], axis=2)\n",
    "        image_list = [img * color_image for img in image_list]\n",
    "\n",
    "        # saturate\n",
    "        image_list = [tf.clip_by_value(img, 0, 1) for img in image_list]\n",
    "\n",
    "        return image_list\n",
    "\n",
    "    def read_image(self, image_path, get_shape=False):\n",
    "#         left_image_o, orig_height, orig_width = self.read_image(\n",
    "#             left_image_path, get_shape=True)\n",
    "        # tf.decode_image does not return the image size, this is an ugly workaround to handle both jpeg and png\n",
    "        path_length = len(image_path)\n",
    "        file_extension = image_path[0].split(\".\")[-1]\n",
    "        \n",
    "        if file_extension == \"jpg\":\n",
    "            image = tf.image.decode_jpeg(tf.io.read_file(image_path))\n",
    "        else:\n",
    "            image = tf.image.decode_png(tf.io.read_file(image_path))\n",
    "\n",
    "        orig_height = tf.cast(tf.shape(input=image)[0], \"float32\")\n",
    "        orig_width = tf.cast(tf.shape(input=image)[1], \"float32\")\n",
    "\n",
    "        image = tf.compat.v1.image.convert_image_dtype(image, tf.float32)\n",
    "        image = tf.compat.v1.image.resize(\n",
    "            image, [self.opt.img_height, self.opt.img_width],\n",
    "            tf.compat.v1.image.ResizeMethod.AREA)\n",
    "\n",
    "        if get_shape:\n",
    "            return image, orig_height, orig_width\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kitti_data/2011_09_26/2011_09_26_drive_0020_sync/image_02/data/0000000043.png', 'kitti_data/2011_09_26/2011_09_26_drive_0060_sync/image_02/data/0000000065.png', 'kitti_data/2011_09_26/2011_09_26_drive_0001_sync/image_02/data/0000000085.png']\n"
     ]
    }
   ],
   "source": [
    "filenames_file = opt.train_file\n",
    "data_path = opt.data_dir\n",
    "\n",
    "with open(filenames_file, 'r') as f:\n",
    "    input_queue = f.readlines()\n",
    "split_line = [line.split() for line in input_queue[:-1]]\n",
    "\n",
    "# we load only one image for test, except if we trained a stereo model   \n",
    "left_image_path = [r\"/\".join([data_path, split_line[i][0]]) for i in range(len(split_line))]\n",
    "right_image_path = [r\"/\".join([data_path, split_line[i][1]]) for i in range(len(split_line))] \n",
    "next_left_image_path = [r\"/\".join([data_path, split_line[i][2]]) for i in range(len(split_line))]\n",
    "next_right_image_path = [r\"/\".join([data_path, split_line[i][3]]) for i in range(len(split_line))]\n",
    "cam_intrinsic_path = [r\"/\".join([data_path, split_line[i][4]]) for i in range(len(split_line))]\n",
    "\n",
    "print(left_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often to record tensorboard summaries.\n",
    "SUMMARY_INTERVAL = 100\n",
    "\n",
    "# How often to run a batch through the validation model.\n",
    "VAL_INTERVAL = 2500\n",
    "\n",
    "# How often to save a model checkpoint\n",
    "SAVE_INTERVAL = 2500\n",
    "\n",
    "def fake_parse():\n",
    "    FLAGS = {\n",
    "        'trace': \"./\",\n",
    "        'num_iterations': 300000,\n",
    "        'pretrained_model': '',\n",
    "        'mode': 'flow',\n",
    "        'train_test': 'train',\n",
    "        'retrain': True,\n",
    "        'data_dir': 'kitti_data',\n",
    "        'train_file':'./filenames/kitti_train_files_png_4frames.txt',\n",
    "        'gt_2012_dir': '',\n",
    "        'gt_2015_dir': '',\n",
    "        'batch_size': 4,\n",
    "        'learning_rate': 0.0001,\n",
    "        'num_gpus': 1,\n",
    "        \"img_height\": 256,\n",
    "        \"img_width\": 832,\n",
    "        \"depth_smooth_weight\": 10.0,\n",
    "        \"ssim_weight\": 0.85,\n",
    "        \"flow_smooth_weight\": 10.0,\n",
    "        \"flow_consist_weight\": 0.01,\n",
    "        \"flow_diff_threshold\": 4.0,\n",
    "        'eval_pose': '',\n",
    "        'num_scales': 4,\n",
    "    }\n",
    "\n",
    "    class Struct:\n",
    "        def __init__(self, **entries):\n",
    "            self.__dict__.update(entries)\n",
    "\n",
    "    options = Struct(**FLAGS)\n",
    "    return options\n",
    "\n",
    "opt = fake_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./filenames/kitti_train_files_png_4frames.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-769789a0f486>:15: TextLineReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Readers are not supported when eager execution is enabled. Instead, please use tf.data to get data into your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-4927867b1396>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMonodepthDataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-769789a0f486>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, opt)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#             [filenames_file], shuffle=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0minput_queue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilenames_file\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mline_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextLineReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\io_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, skip_header_lines, name)\u001b[0m\n\u001b[0;32m    369\u001b[0m     rr = gen_io_ops.text_line_reader_v2(skip_header_lines=skip_header_lines,\n\u001b[0;32m    370\u001b[0m                                         name=name)\n\u001b[1;32m--> 371\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTextLineReader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\io_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, reader_ref, supports_serialize)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \"\"\"\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m       raise RuntimeError(\n\u001b[0m\u001b[0;32m    133\u001b[0m           \u001b[1;34m\"Readers are not supported when eager execution is enabled. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m           \"Instead, please use tf.data to get data into your model.\")\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Readers are not supported when eager execution is enabled. Instead, please use tf.data to get data into your model."
     ]
    }
   ],
   "source": [
    "dataloader = MonodepthDataloader(opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
