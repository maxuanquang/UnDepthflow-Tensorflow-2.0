{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from fake_parse import fake_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = fake_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_length_tf(t):\n",
    "    return tf.py_func(len, [t], [tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_intrinsics(raw_cam_mat, opt, orig_height, orig_width):\n",
    "    fx = raw_cam_mat[0, 0]\n",
    "    fy = raw_cam_mat[1, 1]\n",
    "    cx = raw_cam_mat[0, 2]\n",
    "    cy = raw_cam_mat[1, 2]\n",
    "    r1 = tf.stack(\n",
    "        [fx * opt.img_width / orig_width, 0, cx * opt.img_width / orig_width])\n",
    "    r2 = tf.stack([\n",
    "        0, fy * opt.img_height / orig_height, cy * opt.img_height / orig_height\n",
    "    ])\n",
    "    r3 = tf.constant([0., 0., 1.])\n",
    "    return tf.stack([r1, r2, r3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_scale_intrinsics(raw_cam_mat, num_scales):\n",
    "    proj_cam2pix = []\n",
    "    # Scale the intrinsics accordingly for each scale\n",
    "    for s in range(num_scales):\n",
    "        fx = raw_cam_mat[0, 0] / (2**s)\n",
    "        fy = raw_cam_mat[1, 1] / (2**s)\n",
    "        cx = raw_cam_mat[0, 2] / (2**s)\n",
    "        cy = raw_cam_mat[1, 2] / (2**s)\n",
    "        r1 = tf.stack([fx, 0, cx])\n",
    "        r2 = tf.stack([0, fy, cy])\n",
    "        r3 = tf.constant([0., 0., 1.])\n",
    "        proj_cam2pix.append(tf.stack([r1, r2, r3]))\n",
    "    proj_cam2pix = tf.stack(proj_cam2pix)\n",
    "    proj_pix2cam = tf.matrix_inverse(proj_cam2pix)\n",
    "    proj_cam2pix.set_shape([num_scales, 3, 3])\n",
    "    proj_pix2cam.set_shape([num_scales, 3, 3])\n",
    "    return proj_cam2pix, proj_pix2cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_intrinsics_matrix(fx, fy, cx, cy):\n",
    "    # Assumes batch input\n",
    "    batch_size = fx.get_shape().as_list()[0]\n",
    "    zeros = tf.zeros_like(fx)\n",
    "    r1 = tf.stack([fx, zeros, cx], axis=1)\n",
    "    r2 = tf.stack([zeros, fy, cy], axis=1)\n",
    "    r3 = tf.constant([0., 0., 1.], shape=[1, 3])\n",
    "    r3 = tf.tile(r3, [batch_size, 1])\n",
    "    intrinsics = tf.stack([r1, r2, r3], axis=1)\n",
    "    return intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(im, intrinsics, out_h, out_w):\n",
    "    # Random scaling\n",
    "    def random_scaling(im, intrinsics):\n",
    "        batch_size, in_h, in_w, _ = im.get_shape().as_list()\n",
    "        scaling = tf.random.uniform([2], 1, 1.15)\n",
    "        x_scaling = scaling[0]\n",
    "        y_scaling = scaling[1]\n",
    "        out_h = tf.cast(in_h * y_scaling, dtype=tf.int32)\n",
    "        out_w = tf.cast(in_w * x_scaling, dtype=tf.int32)\n",
    "        im = tf.compat.v1.image.resize(im, [out_h, out_w], method=tf.compat.v1.image.ResizeMethod.AREA)\n",
    "        fx = intrinsics[:, 0, 0] * x_scaling\n",
    "        fy = intrinsics[:, 1, 1] * y_scaling\n",
    "        cx = intrinsics[:, 0, 2] * x_scaling\n",
    "        cy = intrinsics[:, 1, 2] * y_scaling\n",
    "        intrinsics = make_intrinsics_matrix(fx, fy, cx, cy)\n",
    "        return im, intrinsics\n",
    "\n",
    "    # Random cropping\n",
    "    def random_cropping(im, intrinsics, out_h, out_w):\n",
    "        # batch_size, in_h, in_w, _ = im.get_shape().as_list()\n",
    "        batch_size, in_h, in_w, _ = tf.unstack(tf.shape(input=im))\n",
    "        offset_y = tf.random.uniform(\n",
    "            [1], 0, in_h - out_h + 1, dtype=tf.int32)[0]\n",
    "        offset_x = tf.random.uniform(\n",
    "            [1], 0, in_w - out_w + 1, dtype=tf.int32)[0]\n",
    "        im = tf.image.crop_to_bounding_box(im, offset_y, offset_x, out_h,\n",
    "                                           out_w)\n",
    "        fx = intrinsics[:, 0, 0]\n",
    "        fy = intrinsics[:, 1, 1]\n",
    "        cx = intrinsics[:, 0, 2] - tf.cast(offset_x, dtype=tf.float32)\n",
    "        cy = intrinsics[:, 1, 2] - tf.cast(offset_y, dtype=tf.float32)\n",
    "        intrinsics = make_intrinsics_matrix(fx, fy, cx, cy)\n",
    "        return im, intrinsics\n",
    "\n",
    "    im, intrinsics = random_scaling(im, intrinsics)\n",
    "    im, intrinsics = random_cropping(im, intrinsics, out_h, out_w)\n",
    "    return im, intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonodepthDataloader(object):\n",
    "    \"\"\"monodepth dataloader\"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self.data_path = opt.data_dir\n",
    "        self.opt = opt\n",
    "        filenames_file = opt.train_file\n",
    "\n",
    "        input_queue = tf.train.string_input_producer(\n",
    "            [filenames_file], shuffle=False)\n",
    "        line_reader = tf.TextLineReader()\n",
    "        _, line = line_reader.read(input_queue)\n",
    "\n",
    "        split_line = tf.string_split([line]).values\n",
    "\n",
    "        # we load only one image for test, except if we trained a stereo model\n",
    "        left_image_path = tf.string_join([self.data_path, split_line[0]])\n",
    "        right_image_path = tf.string_join([self.data_path, split_line[1]])\n",
    "        next_left_image_path = tf.string_join([self.data_path, split_line[2]])\n",
    "        next_right_image_path = tf.string_join([self.data_path, split_line[3]])\n",
    "        cam_intrinsic_path = tf.string_join([self.data_path, split_line[4]])\n",
    "\n",
    "        left_image_o, orig_height, orig_width = self.read_image(\n",
    "            left_image_path, get_shape=True)\n",
    "        right_image_o = self.read_image(right_image_path)\n",
    "        next_left_image_o = self.read_image(next_left_image_path)\n",
    "        next_right_image_o = self.read_image(next_right_image_path)\n",
    "\n",
    "        # randomly flip images\n",
    "        do_flip = tf.random_uniform([], 0, 1)\n",
    "        left_image = tf.cond(do_flip > 0.5,\n",
    "                             lambda: tf.image.flip_left_right(right_image_o),\n",
    "                             lambda: left_image_o)\n",
    "        right_image = tf.cond(do_flip > 0.5,\n",
    "                              lambda: tf.image.flip_left_right(left_image_o),\n",
    "                              lambda: right_image_o)\n",
    "        next_left_image = tf.cond(\n",
    "            do_flip > 0.5,\n",
    "            lambda: tf.image.flip_left_right(next_right_image_o),\n",
    "            lambda: next_left_image_o)\n",
    "        next_right_image = tf.cond(\n",
    "            do_flip > 0.5, lambda: tf.image.flip_left_right(next_left_image_o),\n",
    "            lambda: next_right_image_o)\n",
    "\n",
    "        do_flip_fb = tf.random_uniform([], 0, 1)\n",
    "        left_image, right_image, next_left_image, next_right_image = tf.cond(\n",
    "            do_flip_fb > 0.5,\n",
    "            lambda: (next_left_image, next_right_image, left_image, right_image),\n",
    "            lambda: (left_image, right_image, next_left_image, next_right_image)\n",
    "        )\n",
    "\n",
    "        # randomly augment images\n",
    "        #         do_augment  = tf.random_uniform([], 0, 0)\n",
    "        #         image_list = [left_image, right_image, next_left_image, next_right_image]\n",
    "        #         left_image, right_image, next_left_image, next_right_image = tf.cond(do_augment > 0.5, \n",
    "        #                                                                              lambda: self.augment_image_list(image_list), \n",
    "        #                                                                              lambda: image_list)\n",
    "\n",
    "        left_image.set_shape([None, None, 3])\n",
    "        right_image.set_shape([None, None, 3])\n",
    "        next_left_image.set_shape([None, None, 3])\n",
    "        next_right_image.set_shape([None, None, 3])\n",
    "\n",
    "        raw_cam_contents = tf.read_file(cam_intrinsic_path)\n",
    "        last_line = tf.string_split(\n",
    "            [raw_cam_contents], delimiter=\"\\n\").values[-1]\n",
    "        raw_cam_vec = tf.string_to_number(\n",
    "            tf.string_split([last_line]).values[1:])\n",
    "        raw_cam_mat = tf.reshape(raw_cam_vec, [3, 4])\n",
    "        raw_cam_mat = raw_cam_mat[0:3, 0:3]\n",
    "        raw_cam_mat = rescale_intrinsics(raw_cam_mat, opt, orig_height,\n",
    "                                         orig_width)\n",
    "\n",
    "        # Scale and crop augmentation\n",
    "        #         im_batch = tf.concat([tf.expand_dims(left_image, 0), \n",
    "        #                          tf.expand_dims(right_image, 0),\n",
    "        #                          tf.expand_dims(next_left_image, 0),\n",
    "        #                          tf.expand_dims(next_right_image, 0)], axis=3)\n",
    "        #         raw_cam_mat_batch = tf.expand_dims(raw_cam_mat, axis=0)\n",
    "        #         im_batch, raw_cam_mat_batch = data_augmentation(im_batch, raw_cam_mat_batch, self.opt.img_height, self.opt.img_width)\n",
    "        #         left_image, right_image, next_left_image, next_right_image = tf.split(im_batch[0,:,:,:], num_or_size_splits=4, axis=2)\n",
    "        #         raw_cam_mat = raw_cam_mat_batch[0,:,:]\n",
    "\n",
    "        proj_cam2pix, proj_pix2cam = get_multi_scale_intrinsics(raw_cam_mat,\n",
    "                                                                opt.num_scales)\n",
    "\n",
    "        # capacity = min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "        min_after_dequeue = 2048\n",
    "        capacity = min_after_dequeue + 4 * opt.batch_size\n",
    "        self.data_batch = tf.train.shuffle_batch([\n",
    "            left_image, right_image, next_left_image, next_right_image,\n",
    "            proj_cam2pix, proj_pix2cam\n",
    "        ], opt.batch_size, capacity, min_after_dequeue, 10)\n",
    "\n",
    "    def augment_image_pair(self, left_image, right_image):\n",
    "        # randomly shift gamma\n",
    "        random_gamma = tf.random_uniform([], 0.8, 1.2)\n",
    "        left_image_aug = left_image**random_gamma\n",
    "        right_image_aug = right_image**random_gamma\n",
    "\n",
    "        # randomly shift brightness\n",
    "        random_brightness = tf.random_uniform([], 0.5, 2.0)\n",
    "        left_image_aug = left_image_aug * random_brightness\n",
    "        right_image_aug = right_image_aug * random_brightness\n",
    "\n",
    "        # randomly shift color\n",
    "        random_colors = tf.random_uniform([3], 0.8, 1.2)\n",
    "        white = tf.ones([tf.shape(left_image)[0], tf.shape(left_image)[1]])\n",
    "        color_image = tf.stack(\n",
    "            [white * random_colors[i] for i in range(3)], axis=2)\n",
    "        left_image_aug *= color_image\n",
    "        right_image_aug *= color_image\n",
    "\n",
    "        # saturate\n",
    "        left_image_aug = tf.clip_by_value(left_image_aug, 0, 1)\n",
    "        right_image_aug = tf.clip_by_value(right_image_aug, 0, 1)\n",
    "\n",
    "        return left_image_aug, right_image_aug\n",
    "\n",
    "    def augment_image_list(self, image_list):\n",
    "        # randomly shift gamma\n",
    "        random_gamma = tf.random_uniform([], 0.8, 1.2)\n",
    "        image_list = [img**random_gamma for img in image_list]\n",
    "\n",
    "        # randomly shift brightness\n",
    "        random_brightness = tf.random_uniform([], 0.5, 2.0)\n",
    "        image_list = [img * random_brightness for img in image_list]\n",
    "\n",
    "        # randomly shift color\n",
    "        random_colors = tf.random_uniform([3], 0.8, 1.2)\n",
    "        white = tf.ones(\n",
    "            [tf.shape(image_list[0])[0], tf.shape(image_list[0])[1]])\n",
    "        color_image = tf.stack(\n",
    "            [white * random_colors[i] for i in range(3)], axis=2)\n",
    "        image_list = [img * color_image for img in image_list]\n",
    "\n",
    "        # saturate\n",
    "        image_list = [tf.clip_by_value(img, 0, 1) for img in image_list]\n",
    "\n",
    "        return image_list\n",
    "\n",
    "    def read_image(self, image_path, get_shape=False):\n",
    "        # tf.decode_image does not return the image size, this is an ugly workaround to handle both jpeg and png\n",
    "        path_length = string_length_tf(image_path)[0]\n",
    "        file_extension = tf.substr(image_path, path_length - 3, 3)\n",
    "        file_cond = tf.equal(file_extension, 'jpg')\n",
    "\n",
    "        image = tf.cond(\n",
    "            file_cond, lambda: tf.image.decode_jpeg(tf.read_file(image_path)),\n",
    "            lambda: tf.image.decode_png(tf.read_file(image_path)))\n",
    "        orig_height = tf.cast(tf.shape(image)[0], \"float32\")\n",
    "        orig_width = tf.cast(tf.shape(image)[1], \"float32\")\n",
    "\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        image = tf.image.resize_images(\n",
    "            image, [self.opt.img_height, self.opt.img_width],\n",
    "            tf.image.ResizeMethod.AREA)\n",
    "\n",
    "        if get_shape:\n",
    "            return image, orig_height, orig_width\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-7b3e3829a9c9>:10: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\quangmx\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\quangmx\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From C:\\Users\\quangmx\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\quangmx\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-9-7b3e3829a9c9>:11: TextLineReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n",
      "WARNING:tensorflow:From <ipython-input-4-efde55be8e55>:2: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From <ipython-input-9-7b3e3829a9c9>:145: substr_deprecated (from tensorflow.python.ops.string_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.strings.substr` instead of `tf.substr`.\n",
      "WARNING:tensorflow:From <ipython-input-9-7b3e3829a9c9>:66: calling string_split (from tensorflow.python.ops.ragged.ragged_string_ops) with delimiter is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "delimiter is deprecated, please use sep instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-7b3e3829a9c9>:93: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "dataloader = MonodepthDataloader(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1, image_r, image2, image2_r, proj_cam2pix, proj_pix2cam = MonodepthDataloader(opt).data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'shuffle_batch_1:0' shape=(4, 256, 832, 3) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_image1 = tf.split(\n",
    "    axis=0, num_or_size_splits=1, value=image1)\n",
    "split_image2 = tf.split(\n",
    "    axis=0, num_or_size_splits=1, value=image2)\n",
    "split_cam2pix = tf.split(\n",
    "    axis=0, num_or_size_splits=1, value=proj_cam2pix)\n",
    "split_pix2cam = tf.split(\n",
    "    axis=0, num_or_size_splits=1, value=proj_pix2cam)\n",
    "split_image_r = tf.split(\n",
    "    axis=0, num_or_size_splits=1, value=image_r)\n",
    "split_image_r_next = tf.split(\n",
    "    axis=0, num_or_size_splits=1, value=image2_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "image1=split_image1[i]\n",
    "image2=split_image2[i]\n",
    "image1r=split_image_r[i]\n",
    "image2r=split_image_r_next[i]\n",
    "cam2pix=split_cam2pix[i]\n",
    "pix2cam=split_pix2cam[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "def feature_pyramid_flow(image, reuse):\n",
    "    with tf.variable_scope('feature_net_flow'):\n",
    "        with slim.arg_scope(\n",
    "            [slim.conv2d, slim.conv2d_transpose],\n",
    "                weights_regularizer=slim.l2_regularizer(0.0004),\n",
    "                activation_fn=leaky_relu,\n",
    "                variables_collections=[\"flownet\"],\n",
    "                reuse=reuse):\n",
    "            cnv1 = slim.conv2d(image, 16, [3, 3], stride=2, scope=\"cnv1\")\n",
    "            cnv2 = slim.conv2d(cnv1, 16, [3, 3], stride=1, scope=\"cnv2\")\n",
    "            cnv3 = slim.conv2d(cnv2, 32, [3, 3], stride=2, scope=\"cnv3\")\n",
    "            cnv4 = slim.conv2d(cnv3, 32, [3, 3], stride=1, scope=\"cnv4\")\n",
    "            cnv5 = slim.conv2d(cnv4, 64, [3, 3], stride=2, scope=\"cnv5\")\n",
    "            cnv6 = slim.conv2d(cnv5, 64, [3, 3], stride=1, scope=\"cnv6\")\n",
    "            cnv7 = slim.conv2d(cnv6, 96, [3, 3], stride=2, scope=\"cnv7\")\n",
    "            cnv8 = slim.conv2d(cnv7, 96, [3, 3], stride=1, scope=\"cnv8\")\n",
    "            cnv9 = slim.conv2d(cnv8, 128, [3, 3], stride=2, scope=\"cnv9\")\n",
    "            cnv10 = slim.conv2d(cnv9, 128, [3, 3], stride=1, scope=\"cnv10\")\n",
    "            cnv11 = slim.conv2d(cnv10, 192, [3, 3], stride=2, scope=\"cnv11\")\n",
    "            cnv12 = slim.conv2d(cnv11, 192, [3, 3], stride=1, scope=\"cnv12\")\n",
    "\n",
    "            return cnv2, cnv4, cnv6, cnv8, cnv10, cnv12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(_x, alpha=0.1):\n",
    "    pos = tf.nn.relu(_x)\n",
    "    neg = alpha * (_x - abs(_x)) * 0.5\n",
    "\n",
    "    return pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model_pwc_full(image1, image2, feature1, feature2):\n",
    "    with tf.variable_scope('flow_net'):\n",
    "        batch_size, H, W, color_channels = map(int, image1.get_shape()[0:4])\n",
    "\n",
    "        #############################\n",
    "        feature1_1, feature1_2, feature1_3, feature1_4, feature1_5, feature1_6 = feature1\n",
    "        feature2_1, feature2_2, feature2_3, feature2_4, feature2_5, feature2_6 = feature2\n",
    "\n",
    "        cv6 = cost_volumn(feature1_6, feature2_6, d=4)\n",
    "        flow6, _ = optical_flow_decoder_dc(cv6, level=6)\n",
    "\n",
    "        flow6to5 = tf.image.resize_bilinear(flow6,\n",
    "                                            [H / (2**5), (W / (2**5))]) * 2.0\n",
    "        feature2_5w = transformer_old(feature2_5, flow6to5, [H / 32, W / 32])\n",
    "        cv5 = cost_volumn(feature1_5, feature2_5w, d=4)\n",
    "        flow5, _ = optical_flow_decoder_dc(\n",
    "            tf.concat(\n",
    "                [cv5, feature1_5, flow6to5], axis=3), level=5)\n",
    "        flow5 = flow5 + flow6to5\n",
    "\n",
    "        flow5to4 = tf.image.resize_bilinear(flow5,\n",
    "                                            [H / (2**4), (W / (2**4))]) * 2.0\n",
    "        feature2_4w = transformer_old(feature2_4, flow5to4, [H / 16, W / 16])\n",
    "        cv4 = cost_volumn(feature1_4, feature2_4w, d=4)\n",
    "        flow4, _ = optical_flow_decoder_dc(\n",
    "            tf.concat(\n",
    "                [cv4, feature1_4, flow5to4], axis=3), level=4)\n",
    "        flow4 = flow4 + flow5to4\n",
    "\n",
    "        flow4to3 = tf.image.resize_bilinear(flow4,\n",
    "                                            [H / (2**3), (W / (2**3))]) * 2.0\n",
    "        feature2_3w = transformer_old(feature2_3, flow4to3, [H / 8, W / 8])\n",
    "        cv3 = cost_volumn(feature1_3, feature2_3w, d=4)\n",
    "        flow3, _ = optical_flow_decoder_dc(\n",
    "            tf.concat(\n",
    "                [cv3, feature1_3, flow4to3], axis=3), level=3)\n",
    "        flow3 = flow3 + flow4to3\n",
    "\n",
    "        flow3to2 = tf.image.resize_bilinear(flow3,\n",
    "                                            [H / (2**2), (W / (2**2))]) * 2.0\n",
    "        feature2_2w = transformer_old(feature2_2, flow3to2, [H / 4, W / 4])\n",
    "        cv2 = cost_volumn(feature1_2, feature2_2w, d=4)\n",
    "        flow2_raw, f2 = optical_flow_decoder_dc(\n",
    "            tf.concat(\n",
    "                [cv2, feature1_2, flow3to2], axis=3), level=2)\n",
    "        flow2_raw = flow2_raw + flow3to2\n",
    "\n",
    "        flow2 = context_net(tf.concat([flow2_raw, f2], axis=3)) + flow2_raw\n",
    "\n",
    "        flow0_enlarge = tf.image.resize_bilinear(flow2 * 4.0, [H, W])\n",
    "        flow1_enlarge = tf.image.resize_bilinear(flow3 * 4.0, [H // 2, W // 2])\n",
    "        flow2_enlarge = tf.image.resize_bilinear(flow4 * 4.0, [H // 4, W // 4])\n",
    "        flow3_enlarge = tf.image.resize_bilinear(flow5 * 4.0, [H // 8, W // 8])\n",
    "\n",
    "        return flow0_enlarge, flow1_enlarge, flow2_enlarge, flow3_enlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_volumn(feature1, feature2, d=4):\n",
    "    batch_size, H, W, feature_num = map(int, feature1.get_shape()[0:4])\n",
    "    feature2 = tf.pad(feature2, [[0, 0], [d, d], [d, d], [0, 0]], \"CONSTANT\")\n",
    "    cv = []\n",
    "    for i in range(2 * d + 1):\n",
    "        for j in range(2 * d + 1):\n",
    "            cv.append(\n",
    "                tf.reduce_mean(\n",
    "                    feature1 * feature2[:, i:(i + H), j:(j + W), :],\n",
    "                    axis=3,\n",
    "                    keep_dims=True))\n",
    "    return tf.concat(cv, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = feature_pyramid_flow(image1, reuse=True)\n",
    "feature2 = feature_pyramid_flow(image2, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1_1, feature1_2, feature1_3, feature1_4, feature1_5, feature1_6 = feature1\n",
    "feature2_1, feature2_2, feature2_3, feature2_4, feature2_5, feature2_6 = feature2\n",
    "\n",
    "# cv6 = cost_volumn(feature1_6, feature2_6, d=4)\n",
    "feature1 = feature1_6\n",
    "feature2 = feature2_6\n",
    "d=4\n",
    "batch_size, H, W, feature_num = map(int, feature1.get_shape()[0:4])\n",
    "feature2 = tf.pad(feature2, [[0, 0], [d, d], [d, d], [0, 0]], \"CONSTANT\")\n",
    "cv = []\n",
    "for i in range(2 * d + 1):\n",
    "    for j in range(2 * d + 1):\n",
    "        cv.append(\n",
    "            tf.reduce_mean(\n",
    "                feature1 * feature2[:, i:(i + H), j:(j + W), :],\n",
    "                axis=3,\n",
    "                keep_dims=True))\n",
    "ret = tf.concat(cv, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Mean_162:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_163:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_164:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_165:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_166:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_167:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_168:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_169:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_170:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_171:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_172:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_173:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_174:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_175:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_176:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_177:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_178:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_179:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_180:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_181:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_182:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_183:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_184:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_185:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_186:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_187:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_188:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_189:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_190:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_191:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_192:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_193:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_194:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_195:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_196:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_197:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_198:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_199:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_200:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_201:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_202:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_203:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_204:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_205:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_206:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_207:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_208:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_209:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_210:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_211:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_212:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_213:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_214:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_215:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_216:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_217:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_218:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_219:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_220:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_221:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_222:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_223:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_224:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_225:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_226:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_227:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_228:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_229:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_230:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_231:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_232:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_233:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_234:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_235:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_236:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_237:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_238:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_239:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_240:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_241:0' shape=(4, 4, 13, 1) dtype=float32>,\n",
       " <tf.Tensor 'Mean_242:0' shape=(4, 4, 13, 1) dtype=float32>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
